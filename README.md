# nbp

Все логические элементы проектов разбиты в джобы. Это как таски в airflow, только с меньшим функционалом. Каждая джоба выполняет свою атомарную функцию. Например, сбор данных, обучение, валидация модели и подсчет метрик. Это удобно, так как в случае ошибки, не надо будет перезапускать весь пайплайн. История запусков хранится в папке с экспериментом. По нему task_manager понимает, какую таску запустить. Также в папке с экспериментом хранятся логи запуска, requirements.txt, и все результаты работы пайплайна. Это удобно, если надо будет воспроизвести аналогичные результаты. 

При запуске эксперимента постарайтесь зафиксировать все возможные сиды. Примеры как это можно сделать находятся в utils/seed.py

## Как запустить эксперимент через Datasphere Jobs?

Пример работы скрипта находится в папке test/02-02-2024

1) Для того, чтобы начать запускать таски в datasphere jobs, необходимо настроить Datasphere CLI у себя в локальном окружении. Ссылка на [док](https://cloud.yandex.ru/ru/docs/datasphere/concepts/jobs/cli)
2) Добавьте PROJECT_ID и S3_CONNECTIONS в .env. 
3) После этого в нужной папке создайте конфиг. Пример конфига находится в test/02-02-2024/test_config.yaml.
4) Запустите run_experiment.sh конфиг. После этого сгенерируется конфиг для датасферы и запустится задача. Логи, история запусков и остальные файлы будут в папке с экспериментом. 


## Как запустить эксперимент в Datasphere?
Так как сейчас датасфера крашится из-за того, что не может скачать пакеты, можно запустить все пайплайны напрямую из датасферы. 
`! task_manager.py --config <ссылка на конфиг>`